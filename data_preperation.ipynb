{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation\n",
    "\n",
    "### Ian Heung\n",
    "\n",
    "In this notebook, I will use Pandas to first select and combine all the datafiles into a single dataframe, then I will store the data onto my local MySQL server through the use of the package `SQLAlchemy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sourcing\n",
    "\n",
    "The data was obtained from an [online database](https://divvy-tripdata.s3.amazonaws.com/index.html), and the data is made avaible by Motivate International Inc. under this [license](https://divvybikes.com/data-license-agreement). I have downloaded the ridershare data from the past 12 months (August 2023 - July 2024). The code in this notebook is compatible with data from past or future time frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Orgnaization\n",
    "\n",
    "Once data has been downloaded, unzip and save the data to a folder. From there, the code below will combine the .csv files into one single Pandas dataframe. Because of the name formatting, we have to use the `datetime` package to create an array to extract each .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you prefer pip to anaconda\n",
    "!pip install pandas --quiet\n",
    "# restart kernal after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data time frame YYYYMM, change to your needs\n",
    "start = 202308\n",
    "end = 202407\n",
    "\n",
    "start_date = datetime.strptime(str(start), \"%Y%m\")\n",
    "end_date = datetime.strptime(str(end), \"%Y%m\")\n",
    "\n",
    "dates_array = []\n",
    "\n",
    "while start_date <= end_date:\n",
    "    dates_array.append(start_date.strftime(\"%Y%m\"))\n",
    "    start_date = start_date.replace(month = start_date.month % 12 + 1, year = start_date.year + (start_date.month // 12))\n",
    "\n",
    "# data range\n",
    "print(dates_array)\n",
    "print(\"Number of datasets: \", len(dates_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through months_array to extract and save each csv file as a pandas dataframe\n",
    "\n",
    "# file path to data, change to your needs\n",
    "filepath = \"data\"\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# concatenate each individual df vertically\n",
    "for date in dates_array:\n",
    "    df = pd.read_csv(os.path.join(filepath, f\"{date}-divvy-tripdata\", f\"{date}-divvy-tripdata.csv\"))\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify head and tail of the new df\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case there is a problem with data on MySQL server, we save a backup csv in the data folder\n",
    "combined_df.to_csv(os.path.join(filepath, \"combined-divvy-tripdata.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to SQL\n",
    "\n",
    "We will store the data in a MySQL database. This allows us to securely place our data in an environment where we can perform additional processing and queries. SQL is well-suited for handling large datasets. For the purposes of the Google Data Analytics Course, the use of SQL is incorporated into this project as I wanted to explore integrating SQL within a Python environment, though it is ultimately not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy PyMySQL ipython-sql --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from getpass import getpass\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your login info for your SQL server\n",
    "user = \"root\"\n",
    "password = getpass() # used to hide your password\n",
    "\n",
    "conn_str = f\"mysql+pymysql://{user}:{password}@localhost:3306/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ipython-sql` library allows us to directly communicate with MySQL in the JuypterNotebook enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SQL session\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql {conn_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new database for our data\n",
    "%sql CREATE DATABASE IF NOT EXISTS CyclisticDatabase;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created database should show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload data onto our newly created database, we will have to use `SQLAlchemy` engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'mysql+pymysql://{user}:{password}@localhost:3306/CyclisticDatabase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data onto our SQL database, this will take a while\n",
    "combined_df.to_sql('combined_tripdata', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets verify that our newly created dataframe has been uploaded onto our SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql USE CyclisticDatabase;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the first 5 entries\n",
    "%sql SELECT * FROM combined_tripdata LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, the data is prepared for cleaning and proccessing. We will move onto cleaning the data, ensuring the accuracy and validity of our data for further analysis into the business problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc371",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
